---
layout: post
title:  反向传播算法概述
category: 数学 
description: 反向传播算法是神经网络训练算法中很基础的一个算法，其原理不是很复杂，仅仅是用了多元函数偏导的计算规则。但是网上的介绍多是描述性的介绍，因此这篇博客着重过程的推导，一步一步的来进行所谓的反向传播算法。
---
首先，我们要明确的是机器学习中一个很重要的概念，即，目标函数。常见的机器学习问题归根到底，是在解决一个优化问题，有些是凸优化，有些是非凸优化，优化问题准备找个时间专门写一个博客来介绍一下内容，在这里就不多说了。我们回到目标函数中来，在神经网络中，目标函数被称作代价函数或者成本函数，即对于单个样本(x,y)，x代表特征,y代表目标，代价函数为

$$J(W,b,x,y) = \frac{1}{2}\|h_{W,b}(x)-y\|_2$$

其中:

$$h_{W,b}(x)$$代表给定输入的变量$x$，经过神经网络模型后的输出为$$h_{W,b}(x)$$，$$W$$，$$b$$是待求解的模型变量
因此，对于所有的样本，我们都可以得到基于这个样本的代价函数，对这些代价函数求和，得到整体代价函数

$$J(W,b) =\frac{1}{n}\sum_{i=1}^n(\frac{1}{2}\|h_{W,b}(x_i)-y_i\|_2)$$

，同时为了防止过拟合，通常会加上参数的规则化项。最后整体代价函数就变成：

$$J(W,b) =\frac{1}{n}\sum_{i=1}^n{\frac{1}{2}\|h_{W,b}(x_i)-y_i\|_2}+\frac{\lambda}{2}\sum{W^2}$$

为了更好的来解释我们的模型，我们首先要约定一些参数的表示方法，在这之前，对于读者来说，我默认是了解过神经网络模型的基本结构，因此不再详细介绍了。

$$W_{i,j}^{(l)}$$表示第$l$层，从上到下第$j$个单元，与第$l+1$层的第$i$个单元的联结参数。

$b_i^{(l)}$表示第$l+1$层，第$i$单元的偏置项。

$S_l$表示第$l$层的单元数目

$$N_l$$表示模型网络层数

因此，可以看出来，对于任意两层之间的参数，都是一个参数二维矩阵。

好，我们回到整体代价函数上来，在经过约定的符号表示以后，整体代价函数可以重新表示为

$$
\begin{equation}
J(W,b) =\frac{1}{n}\sum_{i=1}^n{\frac{1}{2}\|h_{W,b}(x_i)-y_i\|_2}+\frac{\lambda}{2}\sum_{l=1}^{N_l}{\sum_{j=1}^{S_l}{\sum_{i=1}^{S_{l+1}}{(W_{ij}^{(l)})^2}}}
\end{equation}
$$

后面的推导都是基于这个函数，因此务必熟悉这个公式中的符号的含义。
